{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "f93f1bcbd8d9b347febba49476708134689410dbc5888cff8b47ec37e403471a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### comparing embedding spaces\n",
    "\n",
    "I couldn't replicate the openl3 embedding to full precision so I'm going to do some dimensionality reduction to the embedding spaces on some audio samples and see how the two spaces behave wrt each other\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, write a MARL embed\n",
    "import openl3 \n",
    "\n",
    "def load_openl3_model():\n",
    "    return openl3.models.load_audio_embedding_model(input_repr=\"mel128\", embedding_size=512, content_type=\"music\")\n",
    "\n",
    "def openl3_marl(X, sr, model):\n",
    "    assert X.ndim == 3, \"must be shape (batch, channel, sample)\"\n",
    "\n",
    "    embeddings = []\n",
    "    for x in X:\n",
    "        x = x.squeeze(0)\n",
    "        emb, _ = openl3.get_audio_embedding(x, sr, model=model, center=False, verbose=False)\n",
    "        embeddings.append(emb[0])\n",
    "    embeddings = np.stack(embeddings)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, write a torch embed\n",
    "import torch\n",
    "from instrument_recognition.models import torchopenl3\n",
    "\n",
    "def load_torchl3_model():\n",
    "    return torchopenl3.get_model(128, 512)\n",
    "\n",
    "def openl3_torch(X, sr, model):\n",
    "    assert X.ndim == 3\n",
    "    assert sr == 48000\n",
    "\n",
    "    X = torch.from_numpy(X).cuda()\n",
    "\n",
    "    embeddings = model(X).detach().cpu().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "found 831404 entries\n",
      "(('Main System', 57060), ('acoustic guitar', 28536), ('auxiliary percussion', 9768), ('banjo', 1128), ('bassoon', 7536), ('brass section', 2512), ('cello', 19708), ('claps', 272), ('clarinet', 6784), ('clarinet section', 496), ('clean electric guitar', 51816), ('cymbal', 2876), ('distorted electric guitar', 30320), ('double bass', 34244), ('drum machine', 18900), ('drum set', 88268), ('electric bass', 64696), ('female singer', 30580), ('flute', 14420), ('flute section', 212), ('french horn', 2796), ('fx/processed sound', 24456), ('glockenspiel', 2004), ('harmonica', 940), ('harp', 5868), ('male rapper', 2488), ('male singer', 29460), ('mandolin', 11656), ('oboe', 2436), ('piano', 72856), ('piccolo', 216), ('snare drum', 576), ('string section', 4552), ('synthesizer', 39908), ('tabla', 24080), ('tack piano', 3748), ('tambourine', 1380), ('tenor saxophone', 19796), ('timpani', 760), ('trombone', 1380), ('trumpet', 2248), ('trumpet section', 7356), ('tuba', 144), ('vibraphone', 9440), ('viola', 18892), ('viola section', 2296), ('violin', 49420), ('violin section', 4920), ('vocalists', 15200))\n",
      "found 3628 entries\n",
      "(('trombone', 1380), ('trumpet', 2248))\n",
      "found 831404 entries\n",
      "(('Main System', 57060), ('acoustic guitar', 28536), ('auxiliary percussion', 9768), ('banjo', 1128), ('bassoon', 7536), ('brass section', 2512), ('cello', 19708), ('claps', 272), ('clarinet', 6784), ('clarinet section', 496), ('clean electric guitar', 51816), ('cymbal', 2876), ('distorted electric guitar', 30320), ('double bass', 34244), ('drum machine', 18900), ('drum set', 88268), ('electric bass', 64696), ('female singer', 30580), ('flute', 14420), ('flute section', 212), ('french horn', 2796), ('fx/processed sound', 24456), ('glockenspiel', 2004), ('harmonica', 940), ('harp', 5868), ('male rapper', 2488), ('male singer', 29460), ('mandolin', 11656), ('oboe', 2436), ('piano', 72856), ('piccolo', 216), ('snare drum', 576), ('string section', 4552), ('synthesizer', 39908), ('tabla', 24080), ('tack piano', 3748), ('tambourine', 1380), ('tenor saxophone', 19796), ('timpani', 760), ('trombone', 1380), ('trumpet', 2248), ('trumpet section', 7356), ('tuba', 144), ('vibraphone', 9440), ('viola', 18892), ('viola section', 2296), ('violin', 49420), ('violin section', 4920), ('vocalists', 15200))\n",
      "found 3628 entries\n",
      "(('trombone', 1380), ('trumpet', 2248))\n",
      "found 831404 entries\n",
      "(('Main System', 57060), ('acoustic guitar', 28536), ('auxiliary percussion', 9768), ('banjo', 1128), ('bassoon', 7536), ('brass section', 2512), ('cello', 19708), ('claps', 272), ('clarinet', 6784), ('clarinet section', 496), ('clean electric guitar', 51816), ('cymbal', 2876), ('distorted electric guitar', 30320), ('double bass', 34244), ('drum machine', 18900), ('drum set', 88268), ('electric bass', 64696), ('female singer', 30580), ('flute', 14420), ('flute section', 212), ('french horn', 2796), ('fx/processed sound', 24456), ('glockenspiel', 2004), ('harmonica', 940), ('harp', 5868), ('male rapper', 2488), ('male singer', 29460), ('mandolin', 11656), ('oboe', 2436), ('piano', 72856), ('piccolo', 216), ('snare drum', 576), ('string section', 4552), ('synthesizer', 39908), ('tabla', 24080), ('tack piano', 3748), ('tambourine', 1380), ('tenor saxophone', 19796), ('timpani', 760), ('trombone', 1380), ('trumpet', 2248), ('trumpet section', 7356), ('tuba', 144), ('vibraphone', 9440), ('viola', 18892), ('viola section', 2296), ('violin', 49420), ('violin section', 4920), ('vocalists', 15200))\n",
      "found 3628 entries\n",
      "(('trombone', 1380), ('trumpet', 2248))\n",
      "0 ... 1 ... 2 ... 3 ... 4 ... 5 ... 6 ... 7 ... 8 ... 9 ... 10 ... 11 ... 12 ... 13 ... 14 ... 15 ... 16 ... 17 ... 18 ... 19 ... 20 ... 21 ... 22 ... 23 ... 24 ... 25 ... 26 ... 27 ... 28 ... 29 ... 30 ... 31 ... 32 ... 33 ... 34 ... 35 ... 36 ... 37 ... 38 ... 39 ... 40 ... 41 ... 42 ... 43 ... 44 ... 45 ... 46 ... 47 ... 48 ... 49 ... 50 ... 51 ... 52 ... 53 ... 54 ... 55 ... 56 ... 57 ... 58 ... 59 ... 60 ... 61 ... 62 ... 63 ... 64 ... 65 ... 66 ... 67 ... 68 ... 69 ... 70 ... 71 ... 72 ... 73 ... 74 ... 75 ... 76 ... 77 ... 78 ... 79 ... 80 ... 81 ... 82 ... 83 ... 84 ... 85 ... 86 ... 87 ... 88 ... 89 ... 90 ... 91 ... 92 ... 93 ... 94 ... 95 ... 96 ... 97 ... 98 ... 99 ... 100 ... 101 ... 102 ... 103 ... 104 ... 105 ... 106 ... 107 ... 108 ... 109 ... 110 ... 111 ... 112 ... 113 ... 114 ... 115 ... 116 ... 117 ... 118 ... 119 ... 120 ... 121 ... 122 ... 123 ... 124 ... 125 ... 126 ... 127 ... 128 ... 129 ... 130 ... 131 ... 132 ... 133 ... 134 ... 135 ... 136 ... 137 ... 138 ... 139 ... 140 ... 141 ... 142 ... 143 ... 144 ... 145 ... 146 ... 147 ... 148 ... 149 ... 150 ... 151 ... 152 ... 153 ... 154 ... 155 ... 156 ... 157 ... 158 ... 159 ... 160 ... 161 ... 162 ... 163 ... 164 ... 165 ... 166 ... 167 ... 168 ... 169 ... 170 ... 171 ... 172 ... 173 ... 174 ... 175 ... 176 ... 177 ... 178 ... 179 ... 180 ... 181 ... 182 ... 183 ... 184 ... 185 ... 186 ... 187 ... 188 ... 189 ... 190 ... 191 ... 192 ... 193 ... 194 ... 195 ... 196 ... 197 ... 198 ... 199 ... 200 ... "
     ]
    }
   ],
   "source": [
    "from instrument_recognition.datasets import BaseDataModule\n",
    "import tqdm\n",
    "\n",
    "max_samples = 200\n",
    "\n",
    "def debatch(batch):\n",
    "    for k,v in batch.items():\n",
    "        if isinstance(v, list):\n",
    "            batch[k] = v[0]\n",
    "    return batch\n",
    "        \n",
    "\n",
    "path_to_data = \"/home/hugo/CHONK/data/mdb-hop-0.25-chunk-1-AUGMENTED/splits\"\n",
    "dm = BaseDataModule(path_to_data=path_to_data, batch_size=1, num_workers=1, use_embeddings=False, class_subset=['trombone', 'trumpet'])\n",
    "dm.setup()\n",
    "\n",
    "# load models\n",
    "torch_model = load_torchl3_model()\n",
    "torch_model.eval()\n",
    "torch_model.cuda()\n",
    "\n",
    "marl_model = load_openl3_model()\n",
    "\n",
    "# get the validation set bc its smaller\n",
    "dl = dm.train_dataloader()\n",
    "embeddings = []\n",
    "\n",
    "for idx, entry in enumerate(dl):\n",
    "    entry = debatch(entry)\n",
    "    \n",
    "    torch_embedding = openl3_torch(entry['X'].numpy(), entry['sr'], torch_model)\n",
    "    marl_embedding = openl3_marl(entry['X'].numpy(), entry['sr'], marl_model)\n",
    "\n",
    "    embeddings.append(dict(\n",
    "        marl=marl_embedding, \n",
    "        torch=torch_embedding,  \n",
    "        label=entry['label']))\n",
    "\n",
    "    print(idx, '...', end=' ')\n",
    "    if idx >= max_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "marl..\ntorch..\n"
     ]
    }
   ],
   "source": [
    "# plot embedding spaces\n",
    "import os\n",
    "from instrument_recognition.utils.plot import dim_reduce\n",
    "\n",
    "n_components = 2\n",
    "output_dir = '/home/hugo/lab/mono_music_sed/instrument_recognition/notebooks/logs/11.25.20'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "method = 'pca'\n",
    "\n",
    "marl_embeddings = np.concatenate([e['marl'] for e in embeddings], axis=0)\n",
    "torch_embeddings = np.concatenate([e['torch'] for e in embeddings],  axis=0)\n",
    "labels = [e['label'] for e in embeddings]\n",
    "\n",
    "print('marl..')\n",
    "marl_fig = dim_reduce(marl_embeddings,  labels, n_components=n_components, method=method, title='marl openl3')\n",
    "print('torch..')\n",
    "torch_fig = dim_reduce(torch_embeddings,  labels, n_components=n_components, method=method, title='torch openl3')\n",
    "\n",
    "for f, title in zip([marl_fig, torch_fig], [f'marl_fig_{method}.html', f'torch_fig_{method}.html']):\n",
    "    f.write_html(os.path.join(output_dir, title))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}