models I ran today:

# mlp (512)
- mlp version 5: learning rate 0.0003, dropout 0.5
	- seems to not be learning really well, underfitting and stuf
- mlp version 6: learning rate 0.0003, dropout 0.3
	- the underfitting seems to have decreased significantly, in both training and validation sets
- mlp version 7: learning rate 0.001, dropout 0.5
	- trash
- mlp version 8: learning rate 0.0003, dropout 0.15
	- pretty much the same as version 6
- mlp version 9: added batchnorm 1d layers to openlclassifier
	- dropout: 0.3, lr 0.0003
- mlp version 10: added another layer in the middle
	- same specs as v9 otherwise

# mlp-6144
